{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import csv\r\n",
    "from socket import MSG_MCAST\r\n",
    "from urllib.parse import quote, quote_plus\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "strDate = '20210326'\r\n",
    "endDate = '20210327'\r\n",
    "\r\n",
    "filename = f\"daum_news_{strDate}-{endDate}.csv\"\r\n",
    "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\")\r\n",
    "writer = csv.writer(f)\r\n",
    "\r\n",
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"}\r\n",
    "\r\n",
    "dt_index = pd.date_range(start=strDate, end=endDate)\r\n",
    "dt_list = dt_index.strftime(\"%Y%m%d\").tolist()\r\n",
    "max_page = 10000 # 뉴스 페이지 탭 수 지정\r\n",
    "\r\n",
    "\r\n",
    "for i in dt_list:\r\n",
    "    print('날짜',i)\r\n",
    "    for j in range(1579, max_page):\r\n",
    "        main_url = f\"https://news.daum.net/breakingnews/?page={j}&regDate={i}\" # url 입력\r\n",
    "        res = requests.get(main_url, headers=headers)\r\n",
    " \r\n",
    "        val_url = f\"https://news.daum.net/breakingnews/?page={j+1}&regDate={i}\"\r\n",
    "        val = requests.get(val_url, headers=headers)\r\n",
    "\r\n",
    "\r\n",
    "        if (res.status_code == 200) & (val.status_code == 200):\r\n",
    "            print('page : ',j, main_url, 'status:',res.status_code)\r\n",
    "            soup = BeautifulSoup(res.text, \"lxml\") # soup으로 저장\r\n",
    "            val_soup = BeautifulSoup(val.text, \"lxml\")\r\n",
    "\r\n",
    "            main = soup.find(\"ul\", attrs={\"class\":\"list_news2 list_allnews\"})\r\n",
    "            val_main = val_soup.find(\"ul\", attrs={\"class\":\"list_news2 list_allnews\"})\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "            if main != val_main:\r\n",
    "                news = main.find_all(\"strong\", attrs={\"class\":\"tit_thumb\"})\r\n",
    "                cnt = 0\r\n",
    "                for new in news:\r\n",
    "                    urls = new.select_one(\"a\")[\"href\"]# 페이지에 나와있는 뉴스 URL 변수 입력\r\n",
    "                    # print(urls)\r\n",
    "                    result = requests.get(urls, headers=headers)         # request 로 다시 개별 뉴스 접속\r\n",
    "                    if result.status_code == 200:\r\n",
    "                        news_soup = BeautifulSoup(result.text, \"lxml\")\r\n",
    "                        # 뉴스 제목, 발행시간, 기사본문 저장\r\n",
    "                        title = news_soup.find(\"h3\", attrs={\"tit_view\"}).get_text().strip()\r\n",
    "                        pubdate = news_soup.find(\"span\", attrs={\"num_date\"}).get_text().strip()\r\n",
    "                        text = news_soup.find(\"div\", attrs={\"news_view\"}).get_text().strip()\r\n",
    "                        cnt += 1\r\n",
    "                        print(j,'of',cnt,'번째 기사')\r\n",
    "                        # print(i,j,'of',cnt,'번째 기사', urls,'status:', result.status_code)\r\n",
    "                        writer.writerow([cnt, title, pubdate, urls, text])\r\n",
    "                    else:\r\n",
    "                        print(i,j,'of',cnt,'번째 기사','error_code :',result.status_code, urls)\r\n",
    "                        pass\r\n",
    "            else:\r\n",
    "                news = main.find_all(\"strong\", attrs={\"class\":\"tit_thumb\"})\r\n",
    "                cnt = 0\r\n",
    "                for new in news:\r\n",
    "                    urls = new.select_one(\"a\")[\"href\"]# 페이지에 나와있는 뉴스 URL 변수 입력\r\n",
    "                    # print(urls)\r\n",
    "                    result = requests.get(urls, headers=headers)         # request 로 다시 개별 뉴스 접속\r\n",
    "                    if result.status_code == 200:\r\n",
    "                        news_soup = BeautifulSoup(result.text, \"lxml\")\r\n",
    "                        # 뉴스 제목, 발행시간, 기사본문 저장\r\n",
    "                        title = news_soup.find(\"h3\", attrs={\"tit_view\"}).get_text().strip()\r\n",
    "                        pubdate = news_soup.find(\"span\", attrs={\"num_date\"}).get_text().strip()\r\n",
    "                        text = news_soup.find(\"div\", attrs={\"news_view\"}).get_text().strip()\r\n",
    "                        cnt += 1\r\n",
    "                        print(j,'of',cnt,'번째 기사')\r\n",
    "                        # print(i,j,'of',cnt,'번째 기사', urls,'status:', result.status_code)\r\n",
    "                        writer.writerow([cnt, title, pubdate, urls, text])\r\n",
    "                    else:\r\n",
    "                        print(i,j,'of',cnt,'번째 기사','error_code :',result.status_code, urls)\r\n",
    "                        pass\r\n",
    "               \r\n",
    "                break\r\n",
    "                    \r\n",
    "        else:\r\n",
    "            print(i,'page : ',j,'error_code :',res.status_code, main_url)\r\n",
    "            pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "날짜 20210326\n",
      "page :  1579 https://news.daum.net/breakingnews/?page=1579&regDate=20210326 status: 200\n",
      "1579 of 1 번째 기사\n",
      "1579 of 2 번째 기사\n",
      "1579 of 3 번째 기사\n",
      "1579 of 4 번째 기사\n",
      "1579 of 5 번째 기사\n",
      "1579 of 6 번째 기사\n",
      "1579 of 7 번째 기사\n",
      "1579 of 8 번째 기사\n",
      "1579 of 9 번째 기사\n",
      "1579 of 10 번째 기사\n",
      "1579 of 11 번째 기사\n",
      "1579 of 12 번째 기사\n",
      "1579 of 13 번째 기사\n",
      "1579 of 14 번째 기사\n",
      "1579 of 15 번째 기사\n",
      "page :  1580 https://news.daum.net/breakingnews/?page=1580&regDate=20210326 status: 200\n",
      "1580 of 1 번째 기사\n",
      "1580 of 2 번째 기사\n",
      "1580 of 3 번째 기사\n",
      "1580 of 4 번째 기사\n",
      "1580 of 5 번째 기사\n",
      "1580 of 6 번째 기사\n",
      "1580 of 7 번째 기사\n",
      "1580 of 8 번째 기사\n",
      "1580 of 9 번째 기사\n",
      "1580 of 10 번째 기사\n",
      "1580 of 11 번째 기사\n",
      "1580 of 12 번째 기사\n",
      "1580 of 13 번째 기사\n",
      "날짜 20210327\n",
      "page :  1579 https://news.daum.net/breakingnews/?page=1579&regDate=20210327 status: 200\n",
      "1579 of 1 번째 기사\n",
      "1579 of 2 번째 기사\n",
      "1579 of 3 번째 기사\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "list = [1,2,3,4,5,6,7,8,8,8,8,8]\r\n",
    "print(len(list))\r\n",
    "cnt = 0\r\n",
    "for a in list:\r\n",
    "    cnt += 1\r\n",
    "    if a != b:\r\n",
    "        print(cnt, '같지 않아요','a:',a,'b:', b)\r\n",
    "        b = a\r\n",
    "        a+=1\r\n",
    "    else:\r\n",
    "        print(cnt, '같아요',a, b)\r\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12\n",
      "1 같지 않아요 a: 1 b: 8\n",
      "2 같지 않아요 a: 2 b: 1\n",
      "3 같지 않아요 a: 3 b: 2\n",
      "4 같지 않아요 a: 4 b: 3\n",
      "5 같지 않아요 a: 5 b: 4\n",
      "6 같지 않아요 a: 6 b: 5\n",
      "7 같지 않아요 a: 7 b: 6\n",
      "8 같지 않아요 a: 8 b: 7\n",
      "9 같아요 8 8\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit"
  },
  "interpreter": {
   "hash": "68435f9a382c9677d2dcae99d2d139da3415a3d063b38295be49e8c3f2ac11d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}